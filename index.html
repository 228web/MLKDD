<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>CS438/638 Team Swift</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar48"><img src="images/avatar.png" alt="" /></span>
							<h1 id="title">			 Team Swift</h1>
							<p>					KDD Cup Term Project</p>
							<p>			Applied Machine Learning</p>
							<p>           Professor Wei Ding</p>
							<p>                  Spring 2016</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<!--

								Prologue's nav expects links in one of two formats:

								1. Hash link (scrolls to a different section within the page)

								   <li><a href="#foobar" id="foobar-link" class="icon fa-whatever-icon-you-want skel-layers-ignoreHref"><span class="label">Foobar</span></a></li>

								2. Standard link (sends the user to another page/site)

								   <li><a href="http://foobar.tld" id="foobar-link" class="icon fa-whatever-icon-you-want"><span class="label">Foobar</span></a></li>

							-->
							<ul>
								<li><a href="#top" id="top-link" class="skel-layers-ignoreHref"><span class="icon fa-home">Intro</span></a></li>
								<li><a href="#about" id="about-link" class="skel-layers-ignoreHref"><span class="icon fa-user">About Us</span></a></li>
								<li><a href="#problem" id="problem-link" class="skel-layers-ignoreHref"><span class="icon fa-info">Problem Description</span></a></li>
								<li><a href="#data" id="data-link" class="skel-layers-ignoreHref"><span class="icon fa-filter">Data Description</span></a></li>
								<li><a href="#methods" id="methods-link" class="skel-layers-ignoreHref"><span class="icon fa-code">Methods</span></a></li>
								<li><a href="#design" id="design-link" class="skel-layers-ignoreHref"><span class="icon fa-gears">Experimental Design</span></a></li>
								<li><a href="#github" class="skel-layers-ignoreHref"><span class="icon fa-github">Github</span></a></li>
								<li><a href="reports.html#reports" id="reports-link" class="skel-layers-ignoreHref"><span class="icon fa-calendar">Weekly Meeting Reports</span></a></li>
							</ul>
						</nav>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<h2 class="alt">Welcome to Team Swift's KDD Cup Project Website!
								</h2>
								<br>
								<h3 class="alt">This project is for Professor Ding's Spring 2016 Applied Machine Learning course at
									<strong>UMass Boston</strong>.
								</h3>
							</header>

							<footer>
								<a href="#problem" class="button transparent scrolly">Learn More...</a>
							</footer>

						</div>
					</section>


				<!-- About Us -->
					<section id="about" class="two">
						<div class="container">

							<header>
								<h2>About Us</h2>
							</header>

							<h3>We are Team Swift.</h3>
							<br />

							<!-- <a href="#" class="image featured"><img src="images/pic08.jpg" alt="" /></a> -->

							<p>
								Daniel Manning - Undergraduate Student - <a href="mailto:daniel.manning001@umb.edu">daniel.manning001@umb.edu</a>
							</p>
							<p>
								Iman Rezaei - Undergraduate Student - <a href="mailto:rimanmk@gmail.com">rimanmk@gmail.com</a>
							</p>
							<p>
								Jie Qian - Graduate Student - <a href="mailto:jie.qian001@umb.edu">jie.qian001@umb.edu</a>
							</p>
							<p>
								Fei Wu - Graduate Student - <a href="mailto:wufei523@gmail.com">wufei523@gmail.com</a>
							</p>

						</div>
					</section>


				<!-- Problem Description -->
					<section id="problem" class="three">
						<div class="container">

							<header>
								<h2>Problem Description</h2>
							</header>

							<p>
								This project is based on <a href="https://pslcdatashop.web.cmu.edu/KDDCup/">KDD Cup's 2010 Educational Data Mining Challenge.</a>
							</p>
							<h3>Information about KDD Cup</h3>
							<p>
								Participants competed to find and learn the best model from students’ past behavior so they can predict their future performance,
								specifically on algebraic problems. Accurate predictions can then be used to understand and analyze students’ learning process better
								which can lead to optimizing their learning process.
							</p>
							<p>
								Using a dataset from this competition, we use <strong>predictive modeling</strong>
								to learn whether a student correctly answers algebraic problems on the first attempt.  While performing <strong>data analysis</strong>,
								we noticed that the size of the dataset may be too large for training our model.  As a solution, we perform filtering techniques
								to only include data that we believe is meaningful.  <strong>Feature engineering</strong> is also researched and applied to
								produce more meaningful features.  Our ultimate goal is to train and test <strong>transformable models</strong>
								to predict Correct First Attempts with accuracy and precision.
							</p>

							<h3>Why is our project a good machine learning topic?</h3>

							<p>
								Our project is based on a good machine learning topic because we have have good enough history,
								training samples with appropriate features for the students. Therefore, we can train our machine
								to learn from the past history of the students and make predictions on new cases.
							</p>

							<p>
								Moreover, the higher level concept of a transformable model that is initially created for
								predicting whether if a student answers an algebraic step correct or not can,
								can be also used in many other topics such as in:
								<ul class="default">
									<li>Organizing SAT, GRE and GMAT questions</li>
									<li>Online surveys / personality tests</li>
									<li>Predicting whether a customer clicks on an ad depending on the page, section and content of the ad</li>
								</ul>
							</p>

						</div>
					</section>


				<!-- Data Description -->
					<section id="data" class="two">
						<div class="container">
							<header>
								<h2>Data Description</h2>
							</header>
							<p>
								The dataset used in our project is <strong>Algebra I 2005-2006</strong> from KDD Cup 2010,
								containing 813,661 interaction steps. Descriptions of features can be found
								<a href="https://pslcdatashop.web.cmu.edu/KDDCup/rules_data_format.jsp">on the KDD Data Format page.</a>
								We have applied multiple levels of filtering to exclude	data we see as unnecessary to train our models.
							</p>

							<h3>Filter 1: Duration Time</h3>
							<p>
								<br />
								Through data analysis, we have found that 95% of students spend at most 105 seconds on each interaction step.
								We have excluded interaction steps that exceed this number as it is more likely that future students are going to spend
								no more than 105 seconds on each step. Therefore, we decided to filter our data to be focused on the students with that behavior.
								The following histogram shows this phenomenon:
							</p>
							<img src="images/filter1Graph.png" width=70% />
							<p>
								<br />
								Because a step duration of 0-105 seconds covers 95% of interaction steps, such a step duration is likely to be
								included in the test data.
							</p>
							<p>
								The algorithm is applied in <a href="https://github.com/imanmk/MLKDD/blob/master/Source/DurationTimeFilter.py">Source/DurationTimeFilter.py</a>,
								and outputs to file, <a href="https://github.com/imanmk/MLKDD/blob/master/CSV/traindataNoDuration.csv">traindataNoDuration.csv</a>.
								This file contains 769,300 interaction steps (~95% of original dataset).
							</p>
							<br />
							<h3>Filter 2: Problem Frequency</h3>
							<p>
								<br />
								For our second filter, some small feature engineering is applied to categorize each unique problems by string
								concatenating problem hierarchy with problem name.  For example:
								<table>
									<tr>
										<td><b>Problem Hierarchy</b></td>
										<td><b>Problem Name</b></td>
										<td><b>Unique Identifier</b></td>
									</tr>
									<tr>
										<td>Unit ES_04, Section ES_04-1</td>
										<td>EG40</td>
										<td>Unit ES_04, Section ES_04-1;EG40</td>
									</tr>
								</table>
								Based on the generated unique identifiers for the problems, their problem view totals are calculated, and any
								problems that have a problem view total less than 20 are eliminated from the training set.
								About 3.5% of students in the dataset viewed such problems.  3.5% is a good threshold because
								it is very likely that test data can come from the problems that got viewed from top 96.5% of students.
								This reduces the number of unique problems' view count by ~10%, calculated by the following formula:
								<br />
								<br />
								(sum of Problem View Totals | Problem View Total > 20) / (sum of Problem View Totals) = ~89.66%
								<br />
								<br />
								As a toy example to explain the above formula, consider the following:
								<br />
								<br />
								<table>
									<tr>
										<td><b>Unique Problem</b></td>
										<td><b>Total Unique Problem View</b></td>
									</tr>
									<tr>
										<td>Unit CTA1_06, Section CTA1_06-4;L6FB07</td>
										<td>80</td>
									</tr>
									<tr>
										<td>Unit CTA1_02, Section CTA1_02-2;BH1T35B</td>
										<td>42</td>
									</tr>
									<tr>
										<td>Unit CTA1_02, Section CTA1_02-2;BH1T21C</td>
										<td>19</td>
									</tr>
								</table>
								The formula would then produce:
								<br />
								(80 + 42) / (80 + 42 + 19) = 87%
							</p>
							<p>
								If these Unique Problem Identifiers were sorted by their Problem View Total, the following graph is generated:
							</p>
							<img src="images/uniqueProbTotalViews.png" width=70% />
							<p>
								As you can see above, unique problems with row number < ~ 400 (view count > 20 in our data)
								account for having the highest total number of views.
							</p>
							<p>
								The algorithm is applied in the following files:
								<br />
								<a href="https://github.com/imanmk/MLKDD/blob/master/Source/UniqueProbsTotalViews.py">Source/UniqueProbsTotalViews.py</a>
								<br />
								<a href="https://github.com/imanmk/MLKDD/blob/master/Source/UniqueProbViewFilter.py">Source/UniqueProbViewFilter.py</a>
								<br />
								<a href="https://github.com/imanmk/MLKDD/blob/master/Source/traindataFilteredByView.py">Source/traindataFilteredByView.py</a>,
								<br />
								and outputs to file, <a href="https://github.com/imanmk/MLKDD/blob/master/CSV/traindataFilteredByView20.csv">traindataFilteredByView20.csv</a>.
								This file contains 616,977 interaction steps (~80% of filter 1 dataset,
								<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/traindataNoDuration.csv">traindataNoDuration.csv</a>).
							</p>

							<h3>Final Datasets</h3>
							<p>
								The final datasets generated for our experiments are the following:
							</p>
							<ul class="default">
								<li>
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/test.csv">test.csv</a> is 20% of
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/traindataFilteredByView20.csv">traindataFilteredByView20.csv</a> (Randomly selected)
								</li>
								<li>
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/trainValid.csv">trainValid.csv</a> is 80% of the remaining
								</li>
								<li>
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/train.csv">train.csv</a> is 80% of
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/trainValid.csv">trainValid.csv</a> (Randomly generated)
								</li>
								<li>
									<a href="https://github.com/imanmk/MLKDD/blob/master/CSV/validation.csv">validation.csv</a> is the remaining 20%
								</li>
							</ul>
							<p>
								Therefore, our new <a href="https://github.com/imanmk/MLKDD/blob/master/CSV/train.csv">train.csv</a> has 394,865 rows.
							</p>
						</div>
					</section>


				<!-- Machine Learning Methods -->
					<section id="methods" class="four">
						<div class="container">

							<header>
								<h2>Machine Learning Methods</h2>
							</header>

							<!-- K-Nearest Neighbors -->
							<h3>K-Nearest Neighbors</h3>

							<p>
								The K-Nearest Neighbors (KNN) algorithm is a non parametric lazy learning algorithm.
								It internally stores all the given train data and classifies new test input based on a similarity metric such as distance.
							</p>

							<p>
								KNN predicts the value of an input based on the the closest K trained cases.
								In simple words, let's say in our case we want to predict the CFA on a specific step for a student.
								Let's also assume that K = 3. Then the algorithm looks at the closest 3 students that answered the
								same step measured by a distance function. If out of the closest 3 neighbors 2 students have
								CFA = 1 and one student with CFA = 0 then:
							</p>

							<ol class="default">
								<li>
									If we choose to classify our test input, then the final CFA is the value that the majority
									of the 3 neighbors have. Hence, in our case the predicted value for our input is CFA = 1
								</li>
								<li>
									In KNN regression, the predicted value is the average of the values of the input's 3 closest neighbors in our simple example.
								</li>
							</ol>
							<p>
								<b>Note:</b> It is quite useful to assign weights to the neighbors based on their distances.
								In other words, the closest neighbor to the given input has higher impact on the prediction result compare to the
								neighbors that are farther away from the input. Notably, that is also what we did when we applied KNN algorithm to our data.
							</p>

							<p>
								There are 3 main distance functions that can be used in KNN:
							</p>

							<img src="images/knnDistFuncs.png" />
							<br />
							Source: <a href="http://www.saedsayad.com/k_nearest_neighbors.htm">http://www.saedsayad.com/k_nearest_neighbors.htm</a>
							<br />
							<br />
							<p>
								Choosing the best distance function depends on the datasets. As for high dimensional vectors,
								Manhattan function might work better than the Euclidean distance function.
							</p>

							<p>
								Choosing a "good enough" K is another concern. There is no formula to find the exact best number of neighbors.
								However we can use the <i>Elbow</i> method <i>to find to a reasonable number for K by plotting error vs. k and find the point
								where the error is not decreasing significatly (the elbow part).</i>
							</p>

							<!-- Recommended System -->
							<h3>Recommended System</h3>

							<p>
								Recommended systems perform data analysis to make predictions about the rating a user will
								give to specific items.  Sites like Netflix and Amazon use recommended systems to suggest
								movies or products to their consumers.  Here is an example:
							</p>

							<img src="images/recommended.png" />

							<p>
								<ul class="default">
									<li>All the missing values are predicted based on the existing values.</li>
									<li>User is recommended the movie which they would rate the highest.</li>
								</ul>
							</p>

							<p>
							  This type of model is applied in the published paper from KDD Cup 2010,
								<a href="http://pslcdatashop.org/KDDCup/workshop/papers/KDDCup2010_Toescher_Jahrer.pdf">Collaborative Filtering Applied to Educational Data Mining</a>.
								The following table is a comparison between the above example and this dataset:
							</p>

							<table>
								<tr>
									<td><b>Recommender System</b></td>
									<td><b>KDD Problem</b></td>
								</tr>
								<tr>
									<td>Users</td>
									<td>Students</td>
								</tr>
								<tr>
									<td>Items</td>
									<td>Steps of Problems</td>
								</tr>
								<tr>
									<td>Rating, purchases, etc.</td>
									<td>Correct First Attempt</td>
								</tr>
								<tr>
									<td>User-Item Matrix</td>
									<td>Student-Step Matrix</td>
								</tr>
								<tr>
									<td colspan=2>The resulting matrix is sparse.</td>
								</tr>
								<tr>
									<td colspan=2>Goal: Predict the missing elements.</td>
								</tr>
							</table>

							<h3>Decomposing the Student-Step Matrix</h3>

							<p>
								Find matrix A and B such that the product of the matrices <img src="http://latex.codecogs.com/gif.latex?AB^{T}" border="0" />
								approximates the known elements of C.  We then minimized the error with the following equation:
								<br />
								<img src="http://latex.codecogs.com/gif.latex?min_{A,B}\| C-AB^{T} \|_{C}^{2}" border="0" />
							</p>

							<img src="images/matrixVisual.png" />

							<h3>Prediction</h3>

							<p>
								Predicting CFA's using a student-step matrix based on student IDs and the interaction steps is shown by the
								following diagram and equation:
							</p>

							<img src="images/matrixPred.png" width=70% />
							<br />
							<!-- <img src="http://latex.codecogs.com/gif.latex?\^{c}_{is} = a_{i}^{T} \cdot b_{s}" border="0" width=10% /> -->
							<img src="images/equation.png" />

							<p>
								Because we know the features of student s, and we know the features of step i, we can then predict if
								student s can correctly answer step i.
							</p>

						</div>
					</section>


				<!-- Experimental Design -->
					<section id="design" class="three">
						<div class="container">

							<header>
								<h2>Experimental Design</h2>
							</header>
							<p>
								Please discuss how you will thoroughly test the machine learning method(s) using the datasets you construct.
							</p>

							<h3>K-Nearest Neighbors</h3>

							<p>
								<b>Choosing K:</b> For KNN, we will consider plotting a graph of RMSE vs K and apply the elbow method to
								find a "good enough" number of nearest neighbors.
							</p>

							<p>
								<b>Weight Function:</b> For choosing the right weight to be assigned to neighbors, we are going to experiment with different weights, such as:
								<ul class="default">
									<li><b>Uniform:</b> "All points in each neighborhood are weighted equally."</li>
									<li>
										<b>Distance:</b> "weight points by the inverse of their distance."
										In other words, closer neighbors of the input test point will have more impact on the prediction result.
									</li>
									<li>
										<b>Our own defined weight algorithm:</b> Our team will attempt to experiment with other suitable weighting techniques
										that we can create using our knowledge of existing weight functions.
									</li>
								</ul>
							</p>

							<p>
								<b>Algorithm:</b> There are 5 main algorithms provided by sklearn library to compute the nearest neighbors:
								"ball_tree", "kd_tree", "brute" and "auto". Our team will study and analyse these algorithms in details to choose
								the method that works the best to use it in our KNN model.
							</p>

							<p>
								<b>Distance Function:</b> Euclidean, Manhattan and Minkowski are among the most popular distance functions.
								Choosing the suitable function depends on our data. Therefore, we will experiment with these functions to
								find the metric that suits our data best.
							</p>

							<p>
								<b>n_jobs:</b> The number of jobs / threads to run for neighbors search. If observed that we need to lower the time complexity
								of the search we will set this parameter to "-1", which means the number of jobs created is equal to the number of
								CPU cores we have in our computers.
							</p>

							<h3>Recommended System</h3>

							<p>
								For Recommended System, we will test our model with different combinations of parameters and compare the results.
							</p>

							<p>
								One of the most important parameters is the number of features (N) we want to create for students and steps.
								An appropriate N will improve the accuracy of the prediction; however, a very large N would significantly
								increase the training time.  The number of iterations and learning rate will also affect the result and training
								time.
							</p>

							<p>
								We will train several models and test it using our generated test set, and then tweak parameters as we see fit
								by comparing results.  Regularization will also be used to prevent overfitting.
							</p>

							<ul class="default">
								<li>Features Used from Training Set
									<br />
									<ol class="default">
										<li>Student ID, Problem Hierarchy, Problem Name, Step Name</li>
										<li>Construct New Feature: Step ID</li>
										<li>step_id = problem_hierarchy[i] + '/'+ problem_name[i] + '/'+ step_name[i]</li>
									</ol>
								</li>
								<li>
									Library Used: Factorization Machines in Python:<br />
									<a href="https://github.com/coreylynch/pyFM">https://github.com/coreylynch/pyFM</a>
								</li>
								<br />
								<li>Input Data
									<br />
									<ol class="default">
										<li>List of Student ID and step_id (described above)</li>
										<li>List of Correct First Attempt (CFA)</li>
									</ol>
								</li>
								<li>Data Representation<br />
									<ol class="default">
										<li>
											Lists of standard Python dict objects, where the dict elements map each instance's categorical
											and real valued variables to its values.
										</li>
										<li>
											Use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html">sklearn DictVectorizer</a>
											to convert them to a matrix
										</li>
									</ol>
								</li>
							</ul>

							<p>
								We will then introduce bias including
								<img src="http://latex.codecogs.com/gif.latex?\mu" border="0" /> (overall mean CFA),
								<img src="http://latex.codecogs.com/gif.latex?\mu_{s}" border="0" /> (mean CFA of a certain student s), and
								<img src="http://latex.codecogs.com/gif.latex?\mu_{i}" border="0" /> (mean CFA of a certain step i).
								<br />
								The resulting prediction equation is:
								<br />
								<img src="http://latex.codecogs.com/gif.latex?c_{is} = \mu + \mu_{s} + \mu_{i} + a_{i}^{T}b_{s}" border="0" width=20% />
							</p>

							<p>
								After we train the KNN and Recommender System models, we will use some stacking algorithm to combine the results from each of our models.
								In phase one, the linear blender gave us fairly good results, but we will try more algorithms, both linear and nonlinear ones, in the final phase.
							</p>

							<p>
								<b>Note:</b> Initially, all the “training” and “testing" mentioned above is going to be applied on our train and validation sets respectively. 
								At the end, for the final testing we will use our test set to experiment with the final results.
							</p>
						</div>
					</section>

				<!-- Github Repository -->
					<section id="github" class="three">
						<div class="container">

							<header>
								<h2>Github Repository</h2>
							</header>

							<p>
								Click the button below to visit the online code repository on Github.
							</p>

							<a href="https://github.com/imanmk/MLKDD" class="button scrolly"><span class="blue">MLKDD on GitHub</span></a>

						</div>
					</section>

			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; 2016 Team Swift. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
